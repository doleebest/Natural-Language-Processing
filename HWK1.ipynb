{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4139cc19-dd23-4ffa-b6a2-a1b70a6daec6",
   "metadata": {},
   "source": [
    "<center><h1>CSCI 4140: Natural Language Processing</h1></center>\n",
    "<center><h1>CSCI/DASC 6040: Computational Analysis of Natural Languages</h1></center>\n",
    "\n",
    "<center><h6>Spring 2024</h6></center>\n",
    "<center><h6>Homework 1 - N-gram models</h6></center>\n",
    "<center><h6>Due Sunday, January 21, at 11:59 PM</h6></center>\n",
    "\n",
    "<center><font color='red'>Do not redistribute without the instructor’s written permission.</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d5179-89ef-4186-9af3-ce63ff95cc8b",
   "metadata": {},
   "source": [
    "The learning goals of this assignment are to:\n",
    "\n",
    "- Understand how to compute language model probabilities using maximum likelihood estimation.\n",
    "- Implement back-off.\n",
    "- Have fun using a language model to probabilistically generate texts.\n",
    "- Compare word-level langauage models and character-level language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40408d08-1f30-4286-be61-ac741db46121",
   "metadata": {},
   "source": [
    "# Part 1: N-gram Language model (60 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f41ccf-d54b-4f5d-bd2d-4c8811cd84d1",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "84910e85-68eb-41ab-996f-65cad5d8071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bac8f-9980-4905-8029-1d48434008cd",
   "metadata": {},
   "source": [
    "We'll start by loading the data. The WikiText language modeling dataset is a collection of tokens extracted from the set of verified Good and Featured articles on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "22459dd5-ddf4-4e53-9e4a-570646a15820",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'test': '', 'train': '', 'valid': ''}\n",
    "\n",
    "for data_split in data:\n",
    "    fname = \"wiki.{}.tokens\".format(data_split) # 데이터 분할을 기반으로 파일 이름 생성\n",
    "    with open(fname, 'r') as f_wiki: # 해당 파일을 읽기 모드로 연다.\n",
    "        data[data_split] = f_wiki.read().lower().split() # 파일을 읽어 소문자로 변환, 단어를 토큰으로 분할, data 딕셔너리에 저장\n",
    "\n",
    "vocab = list(set(data['train'])) # train 데이터 분할에 대한 단어 목록 생성, set은 중복 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de2aac-d6c2-4ec2-b0d1-5e2305949705",
   "metadata": {},
   "source": [
    "Now have a look at the data by running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d733449a-e810-4cd1-a03d-e309d41e1dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : ['=', 'valkyria', 'chronicles', 'iii', '=', 'senjō', 'no', 'valkyria', '3', ':'] ...\n",
      "dev : ['=', 'homarus', 'gammarus', '=', 'homarus', 'gammarus', ',', 'known', 'as', 'the'] ...\n",
      "test : ['=', 'robert', '<unk>', '=', 'robert', '<unk>', 'is', 'an', 'english', 'film'] ...\n",
      "first 10 words in vocab: ['groundbreaking', 'farce', 'proteomics', 'deficits', 'missy', 'libretto', 'sacristy', 'bc2', 'flap', '18-']\n"
     ]
    }
   ],
   "source": [
    "print('train : %s ...' % data['train'][:10])\n",
    "print('dev : %s ...' % data['valid'][:10])\n",
    "print('test : %s ...' % data['test'][:10])\n",
    "print('first 10 words in vocab: %s' % vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e25cc-9b86-408a-b6f0-1fb139effc34",
   "metadata": {},
   "source": [
    "## Q1.1: Train N-gram language model (25 pts)\n",
    "\n",
    "Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n",
    "\n",
    "*Input:*\n",
    "+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n",
    "+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(w_2 | w_0, w_1)$.\n",
    "\n",
    "*Output:*\n",
    "+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next word computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(w_3 | w_0,w_1,w_2)$ as well as $p(w_3|w_1,w_2)$ and $p(w_3|w_2)$. \n",
    "\n",
    "Each key should be a single string where the words that form the history have been concatenated using spaces. Given a key, its corresponding value should be a dictionary where each word type in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'w1 w2' should look like:\n",
    "\n",
    "    \n",
    "    lm['w1 w2'] = {'w0': 0.001, 'w1' : 1e-6, 'w2' : 1e-6, 'w3': 0.003, ...}\n",
    "    \n",
    "In this example, we also want to store `lm['w2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n",
    "\n",
    "*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "19adcd85-9e8e-4aae-b32b-05029a953dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_lm(data, order=3):\n",
    "    \"\"\"\n",
    "        Train n-gram language model\n",
    "    \"\"\"\n",
    "    \n",
    "    # pad (order-1) special tokens to the left\n",
    "    # for the first token in the text\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data\n",
    "    lm = defaultdict(Counter)\n",
    "    for i in range(len(data) - order):\n",
    "        '''\n",
    "        IMPLEMENT HERE !\n",
    "        '''\n",
    "        for o in range(0, order+1):  # 0부터 order까지의 크기의 히스토리에 대해 반복하기\n",
    "            history = ' '.join(data[i:i+o])\n",
    "            lm[history][data[i+o]] += 1\n",
    "\n",
    "    # Normalize probabilities for each history\n",
    "    for history, word_counts in lm.items():\n",
    "        total_count = sum(word_counts.values())\n",
    "        for word in word_counts:\n",
    "            word_counts[word] /= total_count\n",
    "\n",
    "    return lm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a035c635-bddb-480b-940a-fc38a0482b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking empty history ...\n",
      "checking probability distributions ...\n",
      "checking lengths of histories ...\n",
      "checking word distribution values ...\n",
      "Congratulations, you passed the ngram check!\n"
     ]
    }
   ],
   "source": [
    "def test_ngram_lm():\n",
    "  \n",
    "    print('checking empty history ...')\n",
    "    lm1 = train_ngram_lm(data['train'], order=1)\n",
    "    assert '' in lm1, \"empty history should be in the language model!\"\n",
    "    \n",
    "    print('checking probability distributions ...')\n",
    "    lm2 = train_ngram_lm(data['train'], order=2)\n",
    "    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n",
    "    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][word] should sum to 1!\"\n",
    "    \n",
    "    print('checking lengths of histories ...')\n",
    "    lm3 = train_ngram_lm(data['train'], order=3)\n",
    "    assert len(set([len(k.split()) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n",
    "    \n",
    "    print('checking word distribution values ...')\n",
    "    assert lm1['']['the'] < 0.064 and lm1['']['the'] > 0.062 and \\\n",
    "           lm2['the']['first'] < 0.017 and lm2['the']['first'] > 0.016 and \\\n",
    "           lm3['the first']['time'] < 0.106 and lm3['the first']['time'] > 0.105, \\\n",
    "           \"values do not match!\"\n",
    "    \n",
    "    print(\"Congratulations, you passed the ngram check!\")\n",
    "    \n",
    "test_ngram_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c298d5-e036-4a1e-953c-a3888336b8bc",
   "metadata": {},
   "source": [
    "## Q1.2: Generate text from n-gram language model (10 pts)\n",
    "\n",
    "Complete the following `generate_text` function based on these input/output requirements:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n",
    "+ **vocab**: vocab is a list of unique word types in the training set, already computed for you during data loading.\n",
    "+ **context**: the input context string that you want to condition your language model on, should be a space-separated string of tokens\n",
    "+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n",
    "+ **num_tok**: number of tokens to be generated following the input context\n",
    "\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ generated text, should be a space-separated string\n",
    "    \n",
    "*Hint:*\n",
    "\n",
    "After getting the next-word distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next word from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9833b768-b01c-4f4e-9bca-70019f18b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, vocab, context=\"he is the\", order=3, num_tok=25):\n",
    "    # The goal is to generate new words following the context\n",
    "    # If context has more tokens than the order of lm, \n",
    "    # generate text that follows the last (order-1) tokens of the context\n",
    "    # and store it in the variable ⁠ history ⁠\n",
    "    order -= 1\n",
    "    history = ' '.join(context.split()[-order:])\n",
    "    \n",
    "    # ⁠ out ⁠ is the list of tokens of context\n",
    "    # you need to append the generated tokens to this list\n",
    "    out = context.split()\n",
    "\n",
    "    for i in range(num_tok):  # generate num_tok tokens\n",
    "        if history not in lm:\n",
    "            break  # Break if history not found in language model\n",
    "\n",
    "        # Get the next-word distribution given history\n",
    "        next_word_probs = lm[history]\n",
    "\n",
    "        # Sample the next word from the distribution\n",
    "        next_word = np.random.choice(list(next_word_probs.keys()), p=list(next_word_probs.values()))\n",
    "\n",
    "        # Append the generated token to the list\n",
    "        out.append(next_word)\n",
    "\n",
    "        # Update history for the next iteration\n",
    "        history = ' '.join(out[-order:])\n",
    "\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83943805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 1\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbab80-83be-4789-833e-ad77eb8b06d5",
   "metadata": {},
   "source": [
    "Now try to generate some texts, generated by ngram language model with different orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74e4f974-09f0-4fea-9067-7dba785dc0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 1\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a21de76f-4864-439d-97a1-d78d250d7358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the 1950s , \" . = = 7 million , the war ii granted 200 homes were examining capitalism ) = = = \" , s.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 2\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ac1484f-00bb-4519-9134-5b447b68b2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the setting of sean paul on \" bassline \" garnered four major \" , not the little rock under the names of minor pieces . to'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 3\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "237ee800-b881-4173-8c95-525213b6136f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the managing director of clean edge , a research and strategy firm in the south to bay city along the m @-@ 6 crosses over division'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 4\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe32855-37be-494e-bb66-b96bdc6b5f5d",
   "metadata": {},
   "source": [
    "## Q1.3 : Evaluate the models (25 pts)\n",
    "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n",
    "\n",
    "Recall perplexity is the inverse probability of the test text\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n",
    "\n",
    "For an n-gram model, perplexity is computed by\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n",
    "\n",
    "To address the numerical issue (underflow), we usually compute\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n",
    "\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n",
    "+ **data**: test data\n",
    "+ **vocab**: the list of unique word types in the training set\n",
    "+ **order**: order of the lm\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ the perplexity of test data\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "87e412a7-2ad8-4a79-a4b5-31bc81295e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "def compute_perplexity(lm, data, vocab, order=3):\n",
    "    \n",
    "    # pad according to order\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data\n",
    "    log_sum = 0\n",
    "\n",
    "    for i in range(len(data) - order):\n",
    "        h, w = ' '.join(data[i: i+order]), data[i+order]\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        \"\"\"\n",
    "        \n",
    "        current_order = order  # Preserve the original order value\n",
    "        \n",
    "        # if h not in lm, back-off to n-1 gram and look up again\n",
    "        while h not in lm and current_order > 1:\n",
    "            current_order -= 1\n",
    "            h = ' '.join(data[i: i+current_order])\n",
    "        \n",
    "        if h in lm:\n",
    "            # Get the distribution for the next word\n",
    "            next_word_dist = lm[h]\n",
    "            \n",
    "            # Calculate the conditional probability\n",
    "            conditional_prob = next_word_dist.get(w, 1/len(vocab))  # Use 1/|V| if w not in lm[h]\n",
    "            \n",
    "            # Accumulate the log probability\n",
    "            log_sum += log(conditional_prob)\n",
    "        else:\n",
    "            # Handle the case where h is not in lm even after back-off\n",
    "            # Use 1/|V| as a default probability\n",
    "            log_sum += log(1/len(vocab))\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = exp(-log_sum / (len(data) - (order - 1)))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02749daa-96b5-4f50-aa1c-0279a6e24cef",
   "metadata": {},
   "source": [
    "Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 795, 203, 141, and 130 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f5293167-78a8-49c6-93dd-cbfdff8c3212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1 ppl 24.499292666887136\n",
      "order 2 ppl 10.368212096425294\n",
      "order 3 ppl 59.27749291482931\n",
      "order 4 ppl 92.59853413144387\n"
     ]
    }
   ],
   "source": [
    "for o in [1, 2, 3, 4]:\n",
    "    lm = train_ngram_lm(data['train'], order=o)\n",
    "    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0559387-d8e1-4415-875d-e4157a47f3c5",
   "metadata": {},
   "source": [
    "# Part 2: Character-level N-gram language model (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc0480-756c-4de2-b675-bca705c8f475",
   "metadata": {},
   "source": [
    "In the lecture, language modeling was defined as the task of predicting the next word in a sequence given the previous words. In this part of the assignment, we will focus on the related problem of predicting the next character or word in a sequence given the previous characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0b20a-6d03-441b-8f58-72fb6f1fb751",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80042540-c566-4933-a5bc-323f5b2ea248",
   "metadata": {},
   "source": [
    "We have to modify how we load the data, by splitting it into characters rather than words. Also, we'll use both upper case and lower case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5f64ffff-e278-4dcf-84d7-9ee11f28580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'test': '', 'train': '', 'valid': ''}\n",
    "\n",
    "for data_split in data:\n",
    "    fname = \"wiki.{}.tokens\".format(data_split)\n",
    "    data[data_split] = list(open(fname, 'r', encoding = 'utf-8').read())\n",
    "\n",
    "vocab = list(set(data['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed614e16-882a-4393-b9af-d3bc7a3f8cba",
   "metadata": {},
   "source": [
    "Now have a look at the data by running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e64409d7-4e54-43ef-81c1-2f3227bb390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : [' ', '\\n', ' ', '=', ' ', 'V', 'a', 'l', 'k', 'y'] ...\n",
      "dev : [' ', '\\n', ' ', '=', ' ', 'H', 'o', 'm', 'a', 'r'] ...\n",
      "test : [' ', '\\n', ' ', '=', ' ', 'R', 'o', 'b', 'e', 'r'] ...\n",
      "first 10 characters in vocab: ['p', 'q', 'リ', 'ვ', 'v', 'R', 't', 'α', 'ớ', '′']\n"
     ]
    }
   ],
   "source": [
    "print('train : %s ...' % data['train'][:10])\n",
    "print('dev : %s ...' % data['valid'][:10])\n",
    "print('test : %s ...' % data['test'][:10])\n",
    "print('first 10 characters in vocab: %s' % vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c252f-6345-4d6e-9c88-e2a17d790bf0",
   "metadata": {},
   "source": [
    "## Q2.1: Train N-gram language model (20 pts)\n",
    "\n",
    "Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n",
    "\n",
    "*Input:*\n",
    "+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n",
    "+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(c_2 | c_0, c_1)$.\n",
    "\n",
    "*Output:*\n",
    "+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next character computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(c_3 | c_0,c_1,c_2)$ as well as $p(c_3|c_1,c_2)$ and $p(c_3|c_2)$. \n",
    "\n",
    "Each key should be a single string where the characters that form the history have been concatenated. Given a key, its corresponding value should be a dictionary where each character in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'c1c2' should look like:\n",
    "\n",
    "    \n",
    "    lm['c1c2'] = {'c0': 0.001, 'c1' : 1e-6, 'c2' : 1e-6, 'c3': 0.003, ...}\n",
    "    \n",
    "In this example, we also want to store `lm['c2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n",
    "\n",
    "*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8f7955f9-38f6-4205-b440-78f458186768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_lm(data, order=3):\n",
    "    \"\"\"\n",
    "        Train n-gram language model\n",
    "    \"\"\"\n",
    "    \n",
    "    # pad (order-1) special tokens to the left\n",
    "    # for the first token in the text\n",
    "    order -= 1\n",
    "    data = ['~'] * order + data # \n",
    "    \n",
    "    lm = defaultdict(Counter)\n",
    "    \n",
    "    for i in range(len(data) - order):\n",
    "        for o in range(0, order+1): # loops through different variations of ngrams\n",
    "            history = ''.join(data[i:i+o]) # concatinates words as described above, word at index i+o exclusive\n",
    "            lm[history][data[i+o]] += 1 # update counter for word aqt index i+1 (aka next word in sequence)\n",
    "\n",
    "    for history in lm:\n",
    "        # normalization\n",
    "        total_count = sum(lm[history].values()) # sum up all counts for a particular history\n",
    "        lm[history] = {w: cnt / total_count for w, cnt in lm[history].items()} # create dict as described above: for each word in history, it becomes the key, and the distribution of the next word becomes the value\n",
    "    return dict(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "486f5c48-8efc-4806-996c-17b1230edc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking empty history ...\n",
      "checking probability distributions ...\n",
      "checking lengths of histories ...\n",
      "checking character distribution values ...\n",
      "Congratulations, you passed the ngram check!\n"
     ]
    }
   ],
   "source": [
    "def test_ngram_lm():\n",
    "  \n",
    "    print('checking empty history ...')\n",
    "    lm1 = train_ngram_lm(data['train'], order=1)\n",
    "    assert '' in lm1, \"empty history should be in the language model!\"\n",
    "    \n",
    "    print('checking probability distributions ...')\n",
    "    lm2 = train_ngram_lm(data['train'], order=2)\n",
    "    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n",
    "    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][character] should sum to 1!\"\n",
    "    \n",
    "    print('checking lengths of histories ...')\n",
    "    lm3 = train_ngram_lm(data['train'], order=3)\n",
    "    assert len(set([len(k) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n",
    "    \n",
    "    print('checking character distribution values ...')\n",
    "    assert lm1['']['t'] < 0.062 and lm1['']['t'] > 0.060 and \\\n",
    "           lm2['t']['h'] < 0.297 and lm2['t']['h'] > 0.296 and \\\n",
    "           lm3['th']['e'] < 0.694 and lm3['th']['e'] > 0.693, \\\n",
    "           \"values do not match!\"\n",
    "    \n",
    "    print(\"Congratulations, you passed the ngram check!\")\n",
    "    \n",
    "test_ngram_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1bfa82-d114-4be7-9c60-e86044c3da95",
   "metadata": {},
   "source": [
    "## Q2.2: Generate text from n-gram language model (10 pts)\n",
    "\n",
    "Complete the following `generate_text` function based on these input/output requirements:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n",
    "+ **vocab**: vocab is a list of unique characters in the training set, already computed for you during data loading.\n",
    "+ **context**: the input context string that you want to condition your language model on, should be a string\n",
    "+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n",
    "+ **num_tok**: number of characters to be generated following the input context\n",
    "\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ generated text, should be a sequence of characters\n",
    "    \n",
    "*Hint:*\n",
    "\n",
    "After getting the next-character distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next character from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "18fa2689-f634-49b0-87aa-40d870fa98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text\n",
    "def generate_text(lm, vocab, context=\"he \", order=3, num_tok=25):\n",
    "    \n",
    "    # The goal is to generate new characters following the context\n",
    "    # If context has more tokens than the order of lm, \n",
    "    # generate text that follows the last (order-1) tokens of the context\n",
    "    # and store it in the variable `history`\n",
    "    order -= 1\n",
    "    history = list(context)[-order:]\n",
    "    # `out` is the list of tokens of context\n",
    "    # you need to append the generated tokens to this list\n",
    "    out = list(context)\n",
    "        \n",
    "    for i in range(num_tok):\n",
    "        # Get the history string to use as the key for lm\n",
    "        history_str = ''.join(history)\n",
    "        \n",
    "        # Check if history_str is in the lm\n",
    "        if history_str in lm:\n",
    "            # Get the distribution for the next character\n",
    "            next_char_dist = lm[history_str]\n",
    "            \n",
    "            # Make sure vocab and probabilities have the same size\n",
    "            vocab_subset = [char for char in vocab if char in next_char_dist]\n",
    "            probabilities = [next_char_dist[char] for char in vocab_subset]\n",
    "            \n",
    "            # Sample the next character from the distribution\n",
    "            next_char = np.random.choice(vocab_subset, p=probabilities)\n",
    "            \n",
    "            # Append the sampled character to the output\n",
    "            out.append(next_char)\n",
    "            \n",
    "            # Update the history by removing the first character and adding the sampled character\n",
    "            history = history[1:] + [next_char]\n",
    "        else:\n",
    "            # If history_str is not in lm, break the loop\n",
    "            break\n",
    "    \n",
    "    # Convert the output list to a string\n",
    "    generated_text = ''.join(out)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8f64b-3bcc-4d54-bc10-8dde3b606c14",
   "metadata": {},
   "source": [
    "Now try to generate some texts, generated by ngram language model with different orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a12a210e-84ff-49cd-b408-2bea13ae1fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 1\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4783bb02-15e8-4e52-bae3-7488d877f209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"he is the t , 's Ulthaff sthambe f\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 2\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4672f49d-c6fa-4ed1-8090-5733cc56eb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is thered tubish hac , thervals'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 3\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3cca5823-8440-417f-aa23-3a54d0000d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is their work up the goes \" wer'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 4\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9dd7c5-d2da-489b-8048-cae71cd8a98e",
   "metadata": {},
   "source": [
    "## Q2.3 : Evaluate the models (20 pts)\n",
    "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n",
    "\n",
    "Recall perplexity is the inverse probability of the test text\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n",
    "\n",
    "For an n-gram model, perplexity is computed by\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n",
    "\n",
    "To address the numerical issue (underflow), we usually compute\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n",
    "\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n",
    "+ **data**: test data\n",
    "+ **vocab**: the list of unique characters in the training set\n",
    "+ **order**: order of the lm\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ the perplexity of test data\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "68bf0607-be63-4e4e-8e98-10ae38c204fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "def compute_perplexity(lm, data, vocab, order=3):\n",
    "    \n",
    "    # pad according to order\n",
    "    order -= 1\n",
    "    data = ['~'] * order + data\n",
    "    log_sum = 0\n",
    "    for i in range(len(data) - order):\n",
    "        h, w = ''.join(data[i: i+order]), data[i+order]\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        # if h not in lm, back-off to n-1 gram and look up again\n",
    "        \"\"\"\n",
    "        current_order = order  # Preserve the original order value\n",
    "        \n",
    "        # if h not in lm, back-off to n-1 gram and look up again\n",
    "        while h not in lm and current_order > 1:\n",
    "            current_order -= 1\n",
    "            h = ' '.join(data[i: i+current_order])\n",
    "        \n",
    "        if h in lm:\n",
    "            # Get the distribution for the next word\n",
    "            next_word_dist = lm[h]\n",
    "            \n",
    "            # Calculate the conditional probability\n",
    "            conditional_prob = next_word_dist.get(w, 1/len(vocab))  # Use 1/|V| if w not in lm[h]\n",
    "            \n",
    "            # Accumulate the log probability\n",
    "            log_sum += log(conditional_prob)\n",
    "        else:\n",
    "            # Handle the case where h is not in lm even after back-off\n",
    "            # Use 1/|V| as a default probability\n",
    "            log_sum += log(1/len(vocab))\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = exp(-log_sum / (len(data) - (order - 1)))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e9830-af0d-4a9e-ab3d-2639155a2279",
   "metadata": {},
   "source": [
    "Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 24.5, 10.4, 6.5, and 4.5 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "96da1177-f2dd-4ad4-8192-bcd622771257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1 ppl 24.499292666887136\n",
      "order 2 ppl 10.36816545724361\n",
      "order 3 ppl 6.511296801517689\n",
      "order 4 ppl 4.462984318065475\n"
     ]
    }
   ],
   "source": [
    "for o in [1, 2, 3, 4]:\n",
    "    lm = train_ngram_lm(data['train'], order=o)\n",
    "    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
